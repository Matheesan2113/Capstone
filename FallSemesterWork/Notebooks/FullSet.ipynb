{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/matheesan_manokaran/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /home/matheesan_manokaran/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "import json\n",
    "import warnings\n",
    "import networkx as nx\n",
    "\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from nltk.corpus import words\n",
    "eng_words = words.words('en')\n",
    "\n",
    "from langdetect import detect\n",
    "# to enforce consistent results, check github langdetect readme\n",
    "from langdetect import DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "\n",
    "import os \n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>TweetText</th>\n",
       "      <th>userid</th>\n",
       "      <th>creationtimestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7515433917751296</td>\n",
       "      <td>Enjoy TSA Pat Down RT Hey everybody Thanksgivi...</td>\n",
       "      <td>194306325</td>\n",
       "      <td>2010-11-24 19:26:33 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18960074751868928</td>\n",
       "      <td>Via RT Breaking News Assange signs book deal W...</td>\n",
       "      <td>86711072</td>\n",
       "      <td>2010-12-26 09:23:28 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17700051287547904</td>\n",
       "      <td>state of heart soul your soul Birthing child m...</td>\n",
       "      <td>39126647</td>\n",
       "      <td>2010-12-22 21:56:35 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21446749583314945</td>\n",
       "      <td>Sweet dreams</td>\n",
       "      <td>23262180</td>\n",
       "      <td>2011-01-02 06:04:38 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18528789873041408</td>\n",
       "      <td>dramatic real Operation Clean-Up Now RT CRAP R...</td>\n",
       "      <td>71496690</td>\n",
       "      <td>2010-12-25 04:49:42 UTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                          TweetText  \\\n",
       "0   7515433917751296  Enjoy TSA Pat Down RT Hey everybody Thanksgivi...   \n",
       "1  18960074751868928  Via RT Breaking News Assange signs book deal W...   \n",
       "2  17700051287547904  state of heart soul your soul Birthing child m...   \n",
       "3  21446749583314945                                       Sweet dreams   \n",
       "4  18528789873041408  dramatic real Operation Clean-Up Now RT CRAP R...   \n",
       "\n",
       "      userid        creationtimestamp  \n",
       "0  194306325  2010-11-24 19:26:33 UTC  \n",
       "1   86711072  2010-12-26 09:23:28 UTC  \n",
       "2   39126647  2010-12-22 21:56:35 UTC  \n",
       "3   23262180  2011-01-02 06:04:38 UTC  \n",
       "4   71496690  2010-12-25 04:49:42 UTC  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('mount/TweetsOnUser.csv', lineterminator='\\n', low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLS\n"
     ]
    }
   ],
   "source": [
    "tweets = df.TweetText.values.tolist()\n",
    "userIds = df.userid.values.tolist()\n",
    "print(\"PLS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n",
    "\n",
    "for i in range(len(userIds)):\n",
    "    if userIds[i] not in dict:\n",
    "        dict[userIds[i]] = []\n",
    "    for word in str(tweets[i]).split(\" \"):\n",
    "        dict[userIds[i]].append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = list(dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['you!', 'I', 'love', 'xoxo', 'you!', 'I', 'proud', 'of', 'you', 'I', 'love', 'you', 'my', 'day', 'day.', 'After', 'After', 'all', 'all,', 'I', 'was', 'I', 'was', 'born', 'spread', 'I', 'am', 'love', \"I'm\", 'sadness', 'today', 'who', 'I', 'am', \"I'm\", 'thankful', 'everything', 'everyone', 'good', 'and', 'the', 'bad', 'happiness']]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_final)\n",
    "\n",
    "# Create Corpus\n",
    "# type(data_lemmatized)\n",
    "# print (data_lemmatized)\n",
    "texts = data_final\n",
    "\n",
    "print(data_final[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('After', 2), ('I', 7), (\"I'm\", 2), ('all', 1), ('all,', 1), ('am', 2), ('and', 1), ('bad', 1), ('born', 1), ('day', 1), ('day.', 1), ('everyone', 1), ('everything', 1), ('good', 1), ('happiness', 1), ('love', 3), ('my', 1), ('of', 1), ('proud', 1), ('sadness', 1), ('spread', 1), ('thankful', 1), ('the', 1), ('today', 1), ('was', 2), ('who', 1), ('xoxo', 1), ('you', 2), ('you!', 2)]]\n",
      "[(55,\n",
      "  '0.098*\"Santa\" + 0.077*\"San\" + 0.035*\"Francisco\" + 0.023*\"Claus\" + '\n",
      "  '0.019*\"SF\" + 0.017*\"Ho\" + 0.015*\"Diego\" + 0.014*\"Dear\" + 0.010*\"Bay\" + '\n",
      "  '0.008*\"NORAD\"'),\n",
      " (31,\n",
      "  '0.014*\"Vegas\" + 0.010*\"Las\" + 0.009*\"today\" + 0.007*\"Airport\" + 0.007*\"mph\" '\n",
      "  '+ 0.006*\"Rain\" + 0.006*\"mb\" + 0.006*\"is\" + 0.006*\"Wind\" + 0.006*\"closed\"'),\n",
      " (19,\n",
      "  '0.043*\"RT\" + 0.028*\"la\" + 0.018*\"des\" + 0.018*\"les\" + 0.018*\"et\" + '\n",
      "  '0.016*\"pour\" + 0.016*\"du\" + 0.016*\"en\" + 0.015*\"le\" + 0.015*\"un\"'),\n",
      " (96,\n",
      "  '0.042*\"CA\" + 0.030*\"Jobs\" + 0.029*\"Job\" + 0.028*\"DAILY\" + 0.023*\"shark\" + '\n",
      "  '0.015*\"Los\" + 0.014*\"Manager\" + 0.014*\"sharks\" + 0.013*\"Shark\" + '\n",
      "  '0.012*\"feed\"'),\n",
      " (88,\n",
      "  '0.065*\"CBS\" + 0.044*\"Atlanta\" + 0.037*\"FOX\" + 0.035*\"5\" + 0.024*\"AJC\" + '\n",
      "  '0.019*\"11\" + 0.017*\"Georgia\" + 0.014*\"Alive\" + 0.013*\"Pressure\" + '\n",
      "  '0.010*\"Speed\"'),\n",
      " (85,\n",
      "  '0.014*\"RT\" + 0.013*\"Obama\" + 0.008*\"you\" + 0.007*\"will\" + 0.007*\"GOP\" + '\n",
      "  '0.007*\"tax\" + 0.007*\"we\" + 0.005*\"who\" + 0.005*\"the\" + 0.005*\"Palin\"'),\n",
      " (93,\n",
      "  '0.058*\"M\" + 0.047*\"California\" + 0.039*\"Earthquake\" + 0.027*\"trending\" + '\n",
      "  '0.023*\"3\" + 0.017*\"List\" + 0.016*\"guru\" + 0.013*\"active\" + 0.011*\"2\" + '\n",
      "  '0.011*\"alert\"'),\n",
      " (21,\n",
      "  '0.039*\"art\" + 0.035*\"follow\" + 0.031*\"me\" + 0.031*\"FB\" + 0.024*\"special\" + '\n",
      "  '0.023*\"heroes\" + 0.022*\"please\" + 0.021*\"Please\" + 0.021*\"son\" + '\n",
      "  '0.021*\"Tom\"'),\n",
      " (44,\n",
      "  '0.037*\"Denver\" + 0.029*\"Robert\" + 0.029*\"Colorado\" + 0.028*\"Eclipse\" + '\n",
      "  '0.018*\"Pattinson\" + 0.017*\"Lunar\" + 0.016*\"Winter\" + 0.012*\"CO\" + '\n",
      "  '0.011*\"Kristen\" + 0.010*\"Twilight\"'),\n",
      " (54,\n",
      "  '0.074*\"travel\" + 0.071*\"RT\" + 0.041*\"lp\" + 0.034*\"\" + 0.017*\"tips\" + '\n",
      "  '0.015*\"Travel\" + 0.013*\"Juneau\" + 0.008*\"photo\" + 0.008*\"Caribbean\" + '\n",
      "  '0.006*\"NHK\"'),\n",
      " (26,\n",
      "  '0.034*\"RT\" + 0.024*\"der\" + 0.023*\"die\" + 0.017*\"von\" + 0.017*\"für\" + '\n",
      "  '0.016*\"ist\" + 0.015*\"im\" + 0.014*\"auf\" + 0.013*\"mit\" + 0.011*\"Die\"'),\n",
      " (34,\n",
      "  '0.069*\"RT\" + 0.021*\"di\" + 0.012*\"il\" + 0.011*\"M:\" + 0.009*\"xxx\" + '\n",
      "  '0.007*\"che\" + 0.007*\"si\" + 0.007*\"Il\" + 0.006*\"la\" + 0.006*\"sweetie\"'),\n",
      " (83,\n",
      "  '0.043*\"to\" + 0.040*\"Business\" + 0.037*\"business\" + 0.026*\"win\" + '\n",
      "  '0.026*\"Marketing\" + 0.023*\"How\" + 0.014*\"Share\" + 0.013*\"Friends\" + '\n",
      "  '0.012*\"chance\" + 0.012*\"Tips\"'),\n",
      " (15,\n",
      "  '0.013*\"RT\" + 0.009*\"London\" + 0.009*\"will\" + 0.008*\"snow\" + 0.008*\"weather\" '\n",
      "  '+ 0.007*\"England\" + 0.007*\"flight\" + 0.006*\"today\" + 0.006*\"flights\" + '\n",
      "  '0.006*\"travel\"'),\n",
      " (12,\n",
      "  '0.025*\"HQ\" + 0.017*\"Dec\" + 0.010*\"F1\" + 0.009*\"Candidate\" + 0.009*\"Cruises\" '\n",
      "  '+ 0.008*\"gossip\" + 0.008*\"East\" + 0.008*\"South\" + 0.007*\"TF\" + '\n",
      "  '0.007*\"valid\"'),\n",
      " (38,\n",
      "  '0.169*\"video\" + 0.126*\"YouTube\" + 0.014*\"Check\" + 0.012*\"via\" + '\n",
      "  '0.010*\"uploaded\" + 0.009*\"girl\" + 0.009*\"channel\" + 0.008*\"sexy\" + '\n",
      "  '0.008*\"jeans\" + 0.008*\"RT\"'),\n",
      " (57,\n",
      "  '0.021*\"Grey\" + 0.019*\"Madonna\" + 0.017*\"CFL\" + 0.012*\"GC\" + '\n",
      "  '0.011*\"Saskatchewan\" + 0.010*\"Riders\" + 0.010*\"Ibra\" + 0.010*\"McGuinty\" + '\n",
      "  '0.007*\"Roughriders\" + 0.007*\"Cup\"'),\n",
      " (59,\n",
      "  '0.043*\"E\" + 0.030*\"T\" + 0.026*\"card\" + 0.023*\"O\" + 0.021*\"Panamanian\" + '\n",
      "  '0.017*\"A\" + 0.017*\"R\" + 0.016*\"L\" + 0.015*\"S\" + 0.015*\"BANK\"'),\n",
      " (64,\n",
      "  '0.014*\"You\" + 0.008*\"Obama\" + 0.007*\"Out\" + 0.007*\"Will\" + 0.007*\"Tax\" + '\n",
      "  '0.006*\"Up\" + 0.006*\"Can\" + 0.006*\"American\" + 0.005*\"Day\" + 0.005*\"Who\"'),\n",
      " (33,\n",
      "  '0.101*\"RT\" + 0.071*\"Follow\" + 0.057*\"followers\" + 0.047*\"am\" + 0.042*\"Team\" '\n",
      "  '+ 0.042*\"getting\" + 0.040*\"i\" + 0.036*\"Back\" + 0.032*\"FOLLOW\" + '\n",
      "  '0.030*\"day\"')]\n",
      "\n",
      "Perplexity:  -11.997553249408194\n"
     ]
    }
   ],
   "source": [
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# Human readable format of corpus (term-frequency)\n",
    "print([[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]])\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,id2word=id2word,num_topics=100, passes=10, workers=7)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_lemmatized):\n",
    "#     # Init output\n",
    "#     sent_topics_df = pd.DataFrame()\n",
    "\n",
    "#     # Get main topic in each document\n",
    "#     for i, row in enumerate(ldamodel[corpus]):\n",
    "#         print(row) \n",
    "#         topics = []\n",
    "#         row = sorted(row, key=lambda x: (x[1]), reverse=True) \n",
    "#         for j, (topic_num, prop_topic) in enumerate(row):\n",
    "#             if j in [0,1,2]: \n",
    "#                 topics.append(topic_num)\n",
    "#         # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "#         for j, (topic_num, prop_topic) in enumerate(row):\n",
    "# #             if j in [0,1,2]: #top 3 dominant topics\n",
    "#             if j == 0:  # => dominant topic\n",
    "#                 wp = ldamodel.show_topic(topic_num)\n",
    "#                 topic_keywords = \", \".join([word for word, prop in wp])\n",
    "#                 sent_topics_df = sent_topics_df.append(pd.Series([topics, round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "#             else:\n",
    "#                 break\n",
    "#     sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_final):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.6109</td>\n",
       "      <td>RT, you, I, me, I'm, , my, lol, can, up</td>\n",
       "      <td>[you!, I, love, xoxo, you!, I, proud, of, you,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.7433</td>\n",
       "      <td>you, RT, can, life, will, the, who, people, lo...</td>\n",
       "      <td>[H., Jackson, Brown,, Jr, screwdriver, RT, Inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.6729</td>\n",
       "      <td>RT, Obama, you, will, GOP, tax, we, who, the, ...</td>\n",
       "      <td>[opinion, Sen, Cornyn, Repeal, Don’t, Ask,Don’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.4018</td>\n",
       "      <td>you, RT, can, life, will, the, who, people, lo...</td>\n",
       "      <td>[Hope, stepson, hear, people, power, power, is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.2841</td>\n",
       "      <td>RT, game, , NFL, will, out, up, win, the, season</td>\n",
       "      <td>[Follow, Official, Goon, Gang, Djs, Week, 1, T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.4881</td>\n",
       "      <td>RT, , you, out, will, the, now, of, today, we</td>\n",
       "      <td>[charity:water, Paul, 1:1, 1, time, with, me, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>RT, you, I, me, I'm, , my, lol, can, up</td>\n",
       "      <td>[Blue, Laws, Christmas, outlawed, Boston, you,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.6171</td>\n",
       "      <td>you, Christmas, Happy, New, Thanksgiving, Year...</td>\n",
       "      <td>[Pink, Floyd, Wall, concert, last, night, pret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.5559</td>\n",
       "      <td>, RT, health, you, Health, food, cancer, to, h...</td>\n",
       "      <td>[Search, the, best, doctor, near, you, fb, wei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>E, T, card, O, Panamanian, A, R, L, S, BANK</td>\n",
       "      <td>[Prototype, lanzó, su, 2011, e, Ivan, de, Pine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0            25.0              0.6109   \n",
       "1            1             4.0              0.7433   \n",
       "2            2            85.0              0.6729   \n",
       "3            3             4.0              0.4018   \n",
       "4            4            50.0              0.2841   \n",
       "5            5            63.0              0.4881   \n",
       "6            6            25.0              0.4790   \n",
       "7            7            16.0              0.6171   \n",
       "8            8            58.0              0.5559   \n",
       "9            9            59.0              0.4939   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0            RT, you, I, me, I'm, , my, lol, can, up   \n",
       "1  you, RT, can, life, will, the, who, people, lo...   \n",
       "2  RT, Obama, you, will, GOP, tax, we, who, the, ...   \n",
       "3  you, RT, can, life, will, the, who, people, lo...   \n",
       "4   RT, game, , NFL, will, out, up, win, the, season   \n",
       "5      RT, , you, out, will, the, now, of, today, we   \n",
       "6            RT, you, I, me, I'm, , my, lol, can, up   \n",
       "7  you, Christmas, Happy, New, Thanksgiving, Year...   \n",
       "8  , RT, health, you, Health, food, cancer, to, h...   \n",
       "9        E, T, card, O, Panamanian, A, R, L, S, BANK   \n",
       "\n",
       "                                                Text  \n",
       "0  [you!, I, love, xoxo, you!, I, proud, of, you,...  \n",
       "1  [H., Jackson, Brown,, Jr, screwdriver, RT, Inf...  \n",
       "2  [opinion, Sen, Cornyn, Repeal, Don’t, Ask,Don’...  \n",
       "3  [Hope, stepson, hear, people, power, power, is...  \n",
       "4  [Follow, Official, Goon, Gang, Djs, Week, 1, T...  \n",
       "5  [charity:water, Paul, 1:1, 1, time, with, me, ...  \n",
       "6  [Blue, Laws, Christmas, outlawed, Boston, you,...  \n",
       "7  [Pink, Floyd, Wall, concert, last, night, pret...  \n",
       "8  [Search, the, best, doctor, near, you, fb, wei...  \n",
       "9  [Prototype, lanzó, su, 2011, e, Ivan, de, Pine...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "#TER\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv('testForDomTopicMM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
